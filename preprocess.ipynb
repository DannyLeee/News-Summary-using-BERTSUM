{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from transformers import BertTokenizer\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"dictionary\"\"\"\n",
    "DICT_PATH = \"./dataset/TC/Lexicon2003-72k.txt\"\n",
    "df_dict = pd.read_csv(DICT_PATH, encoding=\"cp950\", header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Human_N = '1'\n",
    "Percent = {10:'01', 20:'02', 30:'03'}\n",
    "FILE_NAME = \"PTSND20011107_1\"\n",
    "ANS_PATH = \"./dataset/TC/TestFile_TD_DOS/DocRatio/Human_\" + Human_N + \"/\" + Percent[10] + \"/\" + FILE_NAME + \".txt\"\n",
    "ans_file = open(ANS_PATH)\n",
    "raw_ans = ans_file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_ans = raw_ans.replace(\"a\",\"\").replace(\"\\n\", \"\")\n",
    "ans = raw_ans.split(\" \")\n",
    "ans = ans[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "answer decoder\n",
    "input: index sequence (list of string)\n",
    "output: decode of index (string)\n",
    "\"\"\"\n",
    "def decoder(index):\n",
    "    result = \"\"\n",
    "    for i in index:\n",
    "        result += df_dict.loc[int(i), 0]\n",
    "#     print(\"f\",result)\n",
    "    return result\n",
    "ans = decoder(ans)\n",
    "# ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NEWS_PATH = \"./dataset/TC/TestFile_TD_DOS/\" + FILE_NAME + \".txt\"\n",
    "news_file = open(NEWS_PATH, encoding=\"cp950\")\n",
    "news = news_file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "content preprocess\n",
    "input: content (string), answer (string)\n",
    "output: processed content(string), label (list of 1/0), cls id (list of int),\n",
    "        origin content (list of string), target text (list of string)\n",
    "\"\"\"\n",
    "def content_preprocess(content, ans):\n",
    "    content = content.replace(\" \", \"\")\n",
    "    content_list = content.splitlines()\n",
    "    origin_content = content_list[:] # create a shallow copy, if don't add [:] will get the pointer\n",
    "    result_label = []\n",
    "    segments_ids = []\n",
    "    cls_ids = []\n",
    "    tgt_text_list = []\n",
    "    pos = 0\n",
    "    for i, text in enumerate(content_list):\n",
    "        if (pos < 512):\n",
    "            cls_ids.append(pos)\n",
    "            pos += len(text) + 2 # +2 for [CLS], [SEP]\n",
    "            if (ans.find(text) != -1):\n",
    "                result_label.append(1) # label\n",
    "                tgt_text_list.append(text)\n",
    "            else:\n",
    "                result_label.append(0) # label\n",
    "        content_list[i] = \"[CLS] \" + text + \" [SEP]\"\n",
    "    result_content = ' '.join(content_list)\n",
    "    tgt_text = '<q>'.join(tgt_text_list)\n",
    "    return result_content, result_label, cls_ids, origin_content, tgt_text\n",
    "content, label, cls_ids, origin_content, tgt_text = content_preprocess(news, ans)\n",
    "print(label)\n",
    "print(cls_ids)\n",
    "print(origin_content)\n",
    "print(tgt_text)\n",
    "content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(label))\n",
    "print(len(cls_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "LM = \"hfl/chinese-bert-wwm\"\n",
    "tokenizer = BertTokenizer.from_pretrained(LM)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_dict = tokenizer.encode_plus(content,\n",
    "                                         add_special_tokens = False,\n",
    "                                         return_token_type_ids = False,\n",
    "                                         max_length=512,\n",
    "                                         pad_to_max_length=True,\n",
    "                                         return_tensors='pt',\n",
    "                                         truncation=True)\n",
    "print(bert_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "segement embedding\n",
    "code from BERTSUM\n",
    "input: input_ids (list of int)\n",
    "output: segment embedding (list of int)\n",
    "\"\"\"\n",
    "def create_segment(input_ids):\n",
    "    segments_ids = []\n",
    "    _segs = [-1] + [i for i, t in enumerate(input_ids) if t == tokenizer.vocab['[SEP]']]\n",
    "    segs = [_segs[i] - _segs[i - 1] for i in range(1, len(_segs))]\n",
    "    for i, s in enumerate(segs):\n",
    "        if (i % 2 == 0):\n",
    "            segments_ids += s* [0]\n",
    "        else:\n",
    "            segments_ids += s * [1]\n",
    "    return segments_ids\n",
    "bert_dict['input_ids'][0][511] = tokenizer.vocab['[SEP]']\n",
    "segments_ids = create_segment(bert_dict['input_ids'][0])\n",
    "# print(len(bert_dict['input_ids'][0]))\n",
    "# print(len(segments_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict = {\"src\": bert_dict['input_ids'][0].tolist(), \"labels\": label, \"segs\": segments_ids, 'clss': cls_ids,\n",
    "                       'src_txt': origin_content, \"tgt_txt\": tgt_text}\n",
    "# print(data_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "dataset = []\n",
    "dataset.append(data_dict)\n",
    "torch.save(dataset, \"simple_test.train.0.bert.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'src': [101, 1037, 2118, 1997, 5947, 3076, 2038, 2351, 3053, 2093, 2706, 2044, 1037, 2991, 1999, 4199, 1999, 1037, 6878, 13742, 2886, 1999, 4199, 1012, 102, 101, 4080, 9587, 29076, 1010, 2322, 1010, 2013, 8904, 3449, 9644, 1010, 4307, 1010, 2018, 2069, 2074, 3369, 2005, 1037, 13609, 2565, 1999, 3304, 2043, 1996, 5043, 3047, 1999, 2254, 1012, 102, 101, 2002, 2001, 10583, 2067, 2000, 3190, 3081, 2250, 10771, 2006, 2233, 2322, 1010, 2021, 2002, 2351, 2006, 4465, 1012, 102, 101, 4080, 9587, 29076, 1010, 2322, 1010, 2013, 8904, 3449, 9644, 1010, 4307, 1010, 1037, 2118, 1997, 5947, 3076, 2038, 2351, 3053, 2093, 2706, 2044, 1037, 2991, 1999, 4199, 1999, 1037, 6878, 13742, 102, 101, 2002, 2001, 2579, 2000, 1037, 2966, 4322, 1999, 1996, 3190, 2181, 1010, 2485, 2000, 2010, 2155, 2188, 1999, 8904, 3449, 9644, 1012, 102, 101, 2002, 2351, 2006, 4465, 2012, 7855, 3986, 2902, 1011, 2966, 19684, 1005, 1055, 2436, 14056, 3581, 18454, 6199, 2319, 2758, 1037, 3426, 1997, 2331, 24185, 1050, 1005, 1056, 2022, 2207, 2127, 6928, 2012, 1996, 5700, 1012, 102, 101, 3988, 2610, 4311, 5393, 1996, 2991, 2001, 2019, 4926, 2021, 4614, 2024, 11538, 1996, 6061, 2008, 9587, 29076, 2001, 20114, 1012, 102, 101, 2006, 4465, 1010, 2010, 5542, 9460, 2626, 3784, 1024, 1036, 2023, 2851, 2026, 5542, 4080, 1005, 1055, 3969, 2001, 4196, 2039, 2000, 6014, 1012, 102, 101, 3988, 2610, 4311, 5393, 1996, 2991, 2001, 2019, 4926, 2021, 4614, 2024, 11538, 1996, 6061, 2008, 9587, 29076, 2001, 20114, 102, 101, 1036, 2012, 1996, 2927, 1997, 2254, 2002, 2253, 2000, 4199, 2000, 2817, 7548, 1998, 2006, 1996, 2126, 2188, 2013, 1037, 2283, 2002, 2001, 23197, 4457, 1998, 6908, 2125, 1037, 2871, 6199, 2958, 1998, 2718, 1996, 5509, 2917, 1012, 102, 101, 1036, 2002, 2001, 1999, 1037, 16571, 1998, 1999, 4187, 4650, 2005, 2706, 1012, 1005, 102, 101, 13723, 20073, 1010, 2040, 2056, 2016, 2003, 1037, 2485, 2155, 2767, 1010, 2409, 2026, 9282, 2166, 1010, 2008, 9587, 29076, 2018, 2069, 2042, 1999, 1996, 2406, 2005, 2416, 2847, 2043, 1996, 5043, 3047, 1012, 102, 101, 2016, 2056, 2002, 2001, 2001, 2894, 2012, 1996, 2051, 1997, 1996, 6884, 6101, 1998, 3167, 5167, 2020, 7376, 1012, 102, 101, 2016, 2794, 2008, 2002, 2001, 1999, 1037, 2512, 1011, 2966, 2135, 10572, 16571, 1010, 2383, 4265, 3809, 8985, 1998, 4722, 9524, 1012, 102, 101, 9587, 29076, 2001, 1037, 2353, 1011, 2095, 5446, 2350, 2013, 8904, 3449, 9644, 1010, 5665, 1012, 1010, 2040, 2001, 8019, 1999, 1037, 13609, 1011, 2146, 2565, 2012, 2198, 9298, 4140, 2118, 1012, 102, 101, 9587, 29076, 6272, 2000, 1996, 2082, 1005, 1055, 3127, 1997, 1996, 13201, 16371, 13577, 1010, 4311, 1996, 3190, 10969, 2040, 6866, 1037, 3696, 2648, 1037, 2311, 3752, 1036, 11839, 2005, 9587, 29076, 1012, 1005, 102, 101, 1996, 13577, 1005, 1055, 5947, 3127, 2623, 4465, 5027, 3081, 10474, 2008, 1037, 3986, 2326, 2097, 2022, 2218, 2006, 3721, 2000, 3342, 9587, 29076, 1012, 102], 'labels': [0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'segs': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'clss': [0, 25, 57, 78, 112, 136, 174, 197, 223, 245, 285, 301, 337, 358, 382, 416, 452], 'src_txt': ['a university of iowa student has died nearly three months after a fall in rome in a suspected robbery attack in rome .', 'andrew mogni , 20 , from glen ellyn , illinois , had only just arrived for a semester program in italy when the incident happened in january .', 'he was flown back to chicago via air ambulance on march 20 , but he died on sunday .', 'andrew mogni , 20 , from glen ellyn , illinois , a university of iowa student has died nearly three months after a fall in rome in a suspected robbery', 'he was taken to a medical facility in the chicago area , close to his family home in glen ellyn .', \"he died on sunday at northwestern memorial hospital - medical examiner 's office spokesman frank shuftan says a cause of death wo n't be released until monday at the earliest .\", 'initial police reports indicated the fall was an accident but authorities are investigating the possibility that mogni was robbed .', \"on sunday , his cousin abby wrote online : ` this morning my cousin andrew 's soul was lifted up to heaven .\", 'initial police reports indicated the fall was an accident but authorities are investigating the possibility that mogni was robbed', '` at the beginning of january he went to rome to study aboard and on the way home from a party he was brutally attacked and thrown off a 40ft bridge and hit the concrete below .', \"` he was in a coma and in critical condition for months . '\", 'paula barnett , who said she is a close family friend , told my suburban life , that mogni had only been in the country for six hours when the incident happened .', 'she said he was was alone at the time of the alleged assault and personal items were stolen .', 'she added that he was in a non-medically induced coma , having suffered serious infection and internal bleeding .', 'mogni was a third-year finance major from glen ellyn , ill. , who was participating in a semester-long program at john cabot university .', \"mogni belonged to the school 's chapter of the sigma nu fraternity , reports the chicago tribune who posted a sign outside a building reading ` pray for mogni . '\", \"the fraternity 's iowa chapter announced sunday afternoon via twitter that a memorial service will be held on campus to remember mogni .\"], 'tgt_txt': 'andrew mogni , 20 , from glen ellyn , illinois , had only just arrived for a semester program when the incident happened in january<q>he was flown back to chicago via air on march 20 but he died on sunday<q>initial police reports indicated the fall was an accident but authorities are investigating the possibility that mogni was robbed<q>his cousin claims he was attacked and thrown 40ft from a bridge'}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "# temp = torch.load(\"./dataset/bert_data_for_BERTSUM/TC/PTS.test.all.bert.pt\")\n",
    "# temp = torch.load(\"../BertSum/bert_data/NewsSummary/PTS.test.0.bert.pt\")\n",
    "temp = torch.load(\"../BertSum/bert_data/cnndm.test.0.bert.pt\")\n",
    "print(temp[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n"
     ]
    }
   ],
   "source": [
    "print(len(temp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "temp = torch.load(\"../BertSum/bert_data/cnndm.train.0.bert.pt\")\n",
    "# temp = torch.load(\"./dataset/bert_data_for_BERTSUM/TC/PTS.train.0.bert.pt\")\n",
    "# temp = torch.load(\"./simple_test.train.0.bert.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(temp[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = set()\n",
    "for i, e in enumerate(temp):\n",
    "    if (len(e['src']) != 512):\n",
    "        print(i, \" src != 512\")\n",
    "        r.add(i)\n",
    "    elif (len(e['labels']) != len(e['clss'])):\n",
    "        print(i, \" labels != clss\")\n",
    "        r.add(i)\n",
    "    elif (len(e['segs']) != 512):\n",
    "        print(i, \" segs != 512\")\n",
    "        r.add(i)\n",
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(temp[0]['clss']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## LCSTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./dataset/LCSTS2.0/DATA/PART_II.txt', 'r') as rf:\n",
    "    with open('./dataset/LCSTS2.0/DATA/PART_II.xml', 'w') as wf:\n",
    "        text = rf.readline()\n",
    "        i = 0\n",
    "        while (text != \"\"):\n",
    "            if (text.find(\"<doc id=\") != -1):\n",
    "                wf.write(\"<doc id=\\\"\" + str(i) + \"\\\">\\n\")\n",
    "                i += 1\n",
    "            else:\n",
    "                wf.write(text)\n",
    "            text = rf.readline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.cElementTree as ET\n",
    "tree = ET.parse('./dataset/LCSTS2.0/DATA/PART_II.xml')\n",
    "# tree = ET.parse('./dataset/TC/TestFile_TD_DOS/GoldHair_DocRatio_REF_20Test_Text01.xml')\n",
    "root = tree.getroot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dict = {'summary' : [], 'text' : []}\n",
    "for child in root:\n",
    "#     print(child.tag, \"-\", child.attrib)\n",
    "#     print(child[1].text)\n",
    "#     print(child[2].text)\n",
    "    df_dict['summary'].append(child[1].text.replace('\\n', '').replace(\" \", \"\"))\n",
    "    df_dict['text'].append(child[2].text.replace('\\n', '').replace(\" \", \"\"))\n",
    "df = pd.DataFrame(data=df_dict)\n",
    "df.to_csv(\"./dataset/LCSTS2.0/DATA/PART_II.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"./dataset/LCSTS2.0/DATA/PART_II.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "def _pad(data, pad_id, width=-1):\n",
    "        if (width == -1):\n",
    "            width = max(len(d) for d in data)\n",
    "        rtn_data = [d + [pad_id] * (width - len(d)) for d in data]\n",
    "        return rtn_data\n",
    "pre_clss = [[0, 27, 82, 135, 156, 185, 193, 249, 266, 288, 316, 357, 372, 428, 449, 476],\n",
    "           [0, 16, 28, 65, 101, 125, 154, 194, 235, 282, 306, 339, 382, 425, 445, 483, 510]]\n",
    "clss = torch.tensor(_pad(pre_clss, -1))\n",
    "mask_cls = ~ (clss == -1)\n",
    "clss[clss == -1] = 0\n",
    "print(mask_cls)\n",
    "print(clss)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
