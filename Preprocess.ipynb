{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from transformers import BertTokenizer\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"dictionary\"\"\"\n",
    "DICT_PATH = \"./dataset/TC/Lexicon2003-72k.txt\"\n",
    "df_dict = pd.read_csv(DICT_PATH, encoding=\"cp950\", header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Human_N = '1'\n",
    "Percent = {10:'01', 20:'02', 30:'03'}\n",
    "FILE_NAME = \"PTSND20011107_1\"\n",
    "ANS_PATH = \"./dataset/TC/TestFile_TD_DOS/DocRatio/Human_\" + Human_N + \"/\" + Percent[10] + \"/\" + FILE_NAME + \".txt\"\n",
    "ans_file = open(ANS_PATH)\n",
    "raw_ans = ans_file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_ans = raw_ans.replace(\"a\",\"\").replace(\"\\n\", \"\")\n",
    "ans = raw_ans.split(\" \")\n",
    "ans = ans[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "answer decoder\n",
    "input: index sequence (list of string)\n",
    "output: decode of index (string)\n",
    "\"\"\"\n",
    "def decoder(index):\n",
    "    result = \"\"\n",
    "    for i in index:\n",
    "        result += df_dict.loc[int(i), 0]\n",
    "#     print(\"f\",result)\n",
    "    return result\n",
    "ans = decoder(ans)\n",
    "# ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NEWS_PATH = \"./dataset/TC/TestFile_TD_DOS/\" + FILE_NAME + \".txt\"\n",
    "news_file = open(NEWS_PATH, encoding=\"cp950\")\n",
    "news = news_file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "content preprocess\n",
    "input: content (string), answer (string)\n",
    "output: processed content(string), label (list of 1/0), cls id (list of int),\n",
    "        origin content (list of string), target text (list of string)\n",
    "\"\"\"\n",
    "def content_preprocess(content, ans):\n",
    "    content = content.replace(\" \", \"\")\n",
    "    content_list = content.splitlines()\n",
    "    origin_content = content_list[:] # create a shallow copy, if don't add [:] will get the pointer\n",
    "    result_label = []\n",
    "    segments_ids = []\n",
    "    cls_ids = []\n",
    "    tgt_text_list = []\n",
    "    pos = 0\n",
    "    for i, text in enumerate(content_list):\n",
    "        if (pos < 512):\n",
    "            cls_ids.append(pos)\n",
    "            pos += len(text) + 2 # +2 for [CLS], [SEP]\n",
    "            if (ans.find(text) != -1):\n",
    "                result_label.append(1) # label\n",
    "                tgt_text_list.append(text)\n",
    "            else:\n",
    "                result_label.append(0) # label\n",
    "        content_list[i] = \"[CLS] \" + text + \" [SEP]\"\n",
    "    result_content = ' '.join(content_list)\n",
    "    tgt_text = '<p>'.join(tgt_text_list)\n",
    "    return result_content, result_label, cls_ids, origin_content, tgt_text\n",
    "content, label, cls_ids, origin_content, tgt_text = content_preprocess(news, ans)\n",
    "print(label)\n",
    "print(cls_ids)\n",
    "print(origin_content)\n",
    "print(tgt_text)\n",
    "content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(label))\n",
    "print(len(cls_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "LM = \"hfl/chinese-bert-wwm\"\n",
    "tokenizer = BertTokenizer.from_pretrained(LM)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_dict = tokenizer.encode_plus(content,\n",
    "                                         add_special_tokens = False,\n",
    "                                         return_token_type_ids = False,\n",
    "                                         max_length=512,\n",
    "                                         pad_to_max_length=True,\n",
    "                                         return_tensors='pt',\n",
    "                                         truncation=True)\n",
    "print(bert_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "segement embedding\n",
    "code from BERTSUM\n",
    "input: input_ids (list of int)\n",
    "output: segment embedding (list of int)\n",
    "\"\"\"\n",
    "def create_segment(input_ids):\n",
    "    segments_ids = []\n",
    "    _segs = [-1] + [i for i, t in enumerate(input_ids) if t == tokenizer.vocab['[SEP]']]\n",
    "    segs = [_segs[i] - _segs[i - 1] for i in range(1, len(_segs))]\n",
    "    for i, s in enumerate(segs):\n",
    "        if (i % 2 == 0):\n",
    "            segments_ids += s* [0]\n",
    "        else:\n",
    "            segments_ids += s * [1]\n",
    "    return segments_ids\n",
    "bert_dict['input_ids'][0][511] = tokenizer.vocab['[SEP]']\n",
    "segments_ids = create_segment(bert_dict['input_ids'][0])\n",
    "# print(len(bert_dict['input_ids'][0]))\n",
    "# print(len(segments_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict = {\"src\": bert_dict['input_ids'][0].tolist(), \"labels\": label, \"segs\": segments_ids, 'clss': cls_ids,\n",
    "                       'src_txt': origin_content, \"tgt_txt\": tgt_text}\n",
    "# print(data_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "dataset = []\n",
    "dataset.append(data_dict)\n",
    "torch.save(dataset, \"simple_test.train.0.bert.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = torch.load(\"simple_test.train.0.bert.pt\")\n",
    "print(temp[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(temp[0]['labels']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "temp = torch.load(\"../BertSum/bert_data/cnndm.train.0.bert.pt\")\n",
    "# temp = torch.load(\"./dataset/bert_data_for_BERTSUM/TC/PTS.train.0.bert.pt\")\n",
    "# temp = torch.load(\"./simple_test.train.0.bert.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(temp[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = set()\n",
    "for i, e in enumerate(temp):\n",
    "    if (len(e['src']) != 512):\n",
    "        print(i, \" src != 512\")\n",
    "        r.add(i)\n",
    "    elif (len(e['labels']) != len(e['clss'])):\n",
    "        print(i, \" labels != clss\")\n",
    "        r.add(i)\n",
    "    elif (len(e['segs']) != 512):\n",
    "        print(i, \" segs != 512\")\n",
    "        r.add(i)\n",
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(temp[0]['clss']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## LCSTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./dataset/LCSTS2.0/DATA/PART_II.txt', 'r') as rf:\n",
    "    with open('./dataset/LCSTS2.0/DATA/PART_II.xml', 'w') as wf:\n",
    "        text = rf.readline()\n",
    "        i = 0\n",
    "        while (text != \"\"):\n",
    "            if (text.find(\"<doc id=\") != -1):\n",
    "                wf.write(\"<doc id=\\\"\" + str(i) + \"\\\">\\n\")\n",
    "                i += 1\n",
    "            else:\n",
    "                wf.write(text)\n",
    "            text = rf.readline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.cElementTree as ET\n",
    "tree = ET.parse('./dataset/LCSTS2.0/DATA/PART_II.xml')\n",
    "# tree = ET.parse('./dataset/TC/TestFile_TD_DOS/GoldHair_DocRatio_REF_20Test_Text01.xml')\n",
    "root = tree.getroot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dict = {'summary' : [], 'text' : []}\n",
    "for child in root:\n",
    "#     print(child.tag, \"-\", child.attrib)\n",
    "#     print(child[1].text)\n",
    "#     print(child[2].text)\n",
    "    df_dict['summary'].append(child[1].text.replace('\\n', '').replace(\" \", \"\"))\n",
    "    df_dict['text'].append(child[2].text.replace('\\n', '').replace(\" \", \"\"))\n",
    "df = pd.DataFrame(data=df_dict)\n",
    "df.to_csv(\"./dataset/LCSTS2.0/DATA/PART_II.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"./dataset/LCSTS2.0/DATA/PART_II.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "def _pad(data, pad_id, width=-1):\n",
    "        if (width == -1):\n",
    "            width = max(len(d) for d in data)\n",
    "        rtn_data = [d + [pad_id] * (width - len(d)) for d in data]\n",
    "        return rtn_data\n",
    "pre_clss = [[0, 27, 82, 135, 156, 185, 193, 249, 266, 288, 316, 357, 372, 428, 449, 476],\n",
    "           [0, 16, 28, 65, 101, 125, 154, 194, 235, 282, 306, 339, 382, 425, 445, 483, 510]]\n",
    "clss = torch.tensor(_pad(pre_clss, -1))\n",
    "mask_cls = ~ (clss == -1)\n",
    "clss[clss == -1] = 0\n",
    "print(mask_cls)\n",
    "print(clss)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
