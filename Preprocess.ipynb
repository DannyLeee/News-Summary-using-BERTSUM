{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from transformers import BertTokenizer\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"dictionary\"\"\"\n",
    "DICT_PATH = \"./dataset/TC/Lexicon2003-72k.txt\"\n",
    "df_dict = pd.read_csv(DICT_PATH, encoding=\"cp950\", header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Human_N = '1'\n",
    "Percent = {10:'01', 20:'02', 30:'03'}\n",
    "FILE_NAME = \"PTSND20011107_1\"\n",
    "ANS_PATH = \"./dataset/TC/TestFile_TD_DOS/DocRatio/Human_\" + Human_N + \"/\" + Percent[10] + \"/\" + FILE_NAME + \".txt\"\n",
    "ans_file = open(ANS_PATH)\n",
    "raw_ans = ans_file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_ans = raw_ans.replace(\"a\",\"\").replace(\"\\n\", \"\")\n",
    "ans = raw_ans.split(\" \")\n",
    "ans = ans[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "answer decoder\n",
    "input: index sequence (list of string)\n",
    "output: decode of index (string)\n",
    "\"\"\"\n",
    "def decoder(index):\n",
    "    result = \"\"\n",
    "    for i in index:\n",
    "        result += df_dict.loc[int(i), 0]\n",
    "#     print(\"f\",result)\n",
    "    return result\n",
    "ans = decoder(ans)\n",
    "# ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NEWS_PATH = \"./dataset/TC/TestFile_TD_DOS/\" + FILE_NAME + \".txt\"\n",
    "news_file = open(NEWS_PATH, encoding=\"cp950\")\n",
    "news = news_file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "content preprocess\n",
    "input: content (string), answer (string)\n",
    "output: processed content(string), label (list of 1/0), cls id (list of int),\n",
    "        origin content (list of string), target text (list of string)\n",
    "\"\"\"\n",
    "def content_preprocess(content, ans):\n",
    "    content = content.replace(\" \", \"\")\n",
    "    content_list = content.splitlines()\n",
    "    origin_content = content_list[:] # create a shallow copy, if don't add [:] will get the pointer\n",
    "    result_label = []\n",
    "    segments_ids = []\n",
    "    cls_ids = []\n",
    "    tgt_text_list = []\n",
    "    pos = 0\n",
    "    for i, text in enumerate(content_list):\n",
    "        if (pos < 512):\n",
    "            cls_ids.append(pos)\n",
    "            pos += len(text) + 2 # +2 for [CLS], [SEP]\n",
    "            if (ans.find(text) != -1):\n",
    "                result_label.append(1) # label\n",
    "                tgt_text_list.append(text)\n",
    "            else:\n",
    "                result_label.append(0) # label\n",
    "        content_list[i] = \"[CLS] \" + text + \" [SEP]\"\n",
    "    result_content = ' '.join(content_list)\n",
    "    tgt_text = '<p>'.join(tgt_text_list)\n",
    "    return result_content, result_label, cls_ids, origin_content, tgt_text\n",
    "content, label, cls_ids, origin_content, tgt_text = content_preprocess(news, ans)\n",
    "print(label)\n",
    "print(cls_ids)\n",
    "print(origin_content)\n",
    "print(tgt_text)\n",
    "content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(label))\n",
    "print(len(cls_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "LM = \"hfl/chinese-bert-wwm\"\n",
    "tokenizer = BertTokenizer.from_pretrained(LM)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_dict = tokenizer.encode_plus(content,\n",
    "                                         add_special_tokens = False,\n",
    "                                         return_token_type_ids = False,\n",
    "                                         max_length=512,\n",
    "                                         pad_to_max_length=True,\n",
    "                                         return_tensors='pt',\n",
    "                                         truncation=True)\n",
    "# print(bert_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "segement embedding\n",
    "code from BERTSUM\n",
    "input: input_ids (list of int)\n",
    "output: segment embedding (list of int)\n",
    "\"\"\"\n",
    "def create_segment(input_ids):\n",
    "    segments_ids = []\n",
    "    _segs = [-1] + [i for i, t in enumerate(input_ids) if t == tokenizer.vocab['[SEP]']]\n",
    "    segs = [_segs[i] - _segs[i - 1] for i in range(1, len(_segs))]\n",
    "    for i, s in enumerate(segs):\n",
    "        if (i % 2 == 0):\n",
    "            segments_ids += s* [0]\n",
    "        else:\n",
    "            segments_ids += s * [1]\n",
    "    return segments_ids\n",
    "bert_dict['input_ids'][0][511] = tokenizer.vocab['[SEP]']\n",
    "segments_ids = create_segment(bert_dict['input_ids'][0])\n",
    "# print(len(bert_dict['input_ids'][0]))\n",
    "# print(len(segments_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict = {\"src\": bert_dict['input_ids'][0].tolist(), \"labels\": label, \"segs\": segments_ids, 'clss': cls_ids,\n",
    "                       'src_txt': origin_content, \"tgt_txt\": tgt_text}\n",
    "# print(data_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "dataset = []\n",
    "dataset.append(data_dict)\n",
    "torch.save(dataset, \"simple_test.train.0.bert.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = torch.load(\"simple_test.train.0.bert.pt\")\n",
    "print(len(temp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(temp[0]['labels']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "# temp = torch.load(\"../BertSum/bert_data/cnndm.train.0.bert.pt\")\n",
    "temp = torch.load(\"./dataset/bert_data_for_BERTSUM/TC/PTS.train.0.bert.pt\")\n",
    "# temp = torch.load(\"./simple_test.train.0.bert.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "set()"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = set()\n",
    "for i, e in enumerate(temp):\n",
    "    if (len(e['src']) != 512):\n",
    "        print(i, \" src != 512\")\n",
    "        r.add(i)\n",
    "    elif (len(e['labels']) != len(e['clss'])):\n",
    "        print(i, \" labels != clss\")\n",
    "        r.add(i)\n",
    "    elif (len(e['segs']) != 512):\n",
    "        print(i, \" segs != 512\")\n",
    "        r.add(i)\n",
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n"
     ]
    }
   ],
   "source": [
    "print(len(temp[0]['clss']))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
